from playwright.async_api import async_playwright, TimeoutError as PlaywrightTimeoutError
import pandas as pd
import asyncio
import logging
import re
from typing import Dict, List

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class Config:
    URL_BASE = 'https://www.trucadao.com.br/venda/caminhoes-usados?tipo=cavalo-mecanico&page=1'
    MAX_PAGINAS = 10
    MAX_BOTOES_POR_PAGINA = 20
    MAX_RETRIES = 3
    TIMEOUT_PADRAO = 60000
    OUTPUT_FILE = 'dados_trucadao_completos.xlsx'

async def extrair_informacoes_tecnicas(texto: str) -> Dict[str, str]:
    campos = {
        "Tipo": "N√£o informado",
        "Marca": "N√£o informado",
        "Modelo": "N√£o informado",
        "Ano": "N√£o informado",
        "Situa√ß√£o": "N√£o informado",
        "Quilometragem": "N√£o informado",
        "Combust√≠vel": "N√£o informado",
        "Cor": "N√£o informado"
    }

    linhas = [l.strip() for l in texto.split('\n') if l.strip()]
    i = 0
    while i < len(linhas):
        linha = linhas[i].lower()
        for campo in campos.keys():
            if campo.lower() in linha:
                if i + 1 < len(linhas):
                    campos[campo] = linhas[i + 1].strip()
                break
        i += 1

    return campos

async def tentar_extrair_preco(pagina, config: Config) -> str:
    # Tenta primeiro com seletor direto
    preco_selectors = [
        "h5.price",
        "h5.MuiTypography-root.MuiTypography-h5.price",  # alternativa exata do HTML
        "h5:has-text('R$')"  # fallback textual
    ]

    for selector in preco_selectors:
        try:
            logger.info(f"üîç Tentando localizar pre√ßo com seletor: {selector}")
            await pagina.wait_for_selector(selector, timeout=config.TIMEOUT_PADRAO)
            preco_raw = await pagina.locator(selector).inner_text()
            if preco_raw:
                logger.info(f"üí∞ Pre√ßo encontrado via seletor: {preco_raw}")
                break
        except Exception:
            continue
    else:
        # Fallback via JavaScript DOM direto
        logger.warning("‚ö†Ô∏è Nenhum seletor funcionou, tentando via JavaScript direto")
        preco_raw = await pagina.evaluate("""
            () => {
                const el = document.querySelector('h5.price');
                return el ? el.innerText : null;
            }
        """)
        if not preco_raw:
            logger.error("‚ùå Pre√ßo n√£o encontrado nem com fallback")
            return "N√£o informado"
        logger.info(f"‚úÖ Pre√ßo via evaluate: {preco_raw}")

    # Limpeza e formata√ß√£o
    try:
        preco_limpo = preco_raw.replace("R$", "").replace("\xa0", "").replace("¬†", "").replace(".", "").replace(",", ".").strip()
        if preco_limpo.replace('.', '').isdigit():
            preco_formatado = f"R$ {float(preco_limpo):,.2f}".replace(".", "X").replace(",", ".").replace("X", ",")
            logger.info(f"üí∞ Pre√ßo extra√≠do: {preco_formatado}")
            return preco_formatado
        else:
            logger.warning(f"‚ö†Ô∏è Pre√ßo n√£o num√©rico ap√≥s limpeza: {preco_limpo}")
    except Exception as e:
        logger.error(f"‚ùå Erro ao formatar pre√ßo: {e}")

    return "N√£o informado"

async def tentar_extrair_dados_tecnicos(pagina, config: Config) -> Dict[str, str]:
    try:
        info_selector = "div[role='tabpanel']:has(div:has-text('Marca'))"
        await pagina.wait_for_selector(info_selector, timeout=config.TIMEOUT_PADRAO)
        informacoes = await pagina.locator(info_selector).inner_text()
        return await extrair_informacoes_tecnicas(informacoes)
    except Exception as e:
        logger.error(f"Erro nos dados t√©cnicos: {e}")
        return {k: "Erro" for k in ["Tipo", "Marca", "Modelo", "Ano", "Situa√ß√£o", "Quilometragem", "Combust√≠vel", "Cor"]}

async def tentar_extrair_revenda(pagina, config: Config) -> str:
    try:
        selector = "span.city"
        el = pagina.locator(selector)

        await el.scroll_into_view_if_needed()
        await el.wait_for(state="visible", timeout=15000)  # timeout menor evita travar o script

        raw_text = await el.inner_text()
        revenda_text = raw_text.replace("Cidade:", "").strip().title()

        logger.info(f"üè™ Localiza√ß√£o da revenda: {revenda_text}")
        return revenda_text
    except Exception as e:
        logger.warning(f"Erro na revenda: {e}")
        return "N√£o informado"

async def extracaoDadosTrucadao(pagina, config: Config) -> List[Dict]:
    dados_coletados = []
    try:
        botoes = pagina.locator('button.button', has_text="Ver an√∫ncio")
        await pagina.wait_for_selector('button.button', timeout=config.TIMEOUT_PADRAO)
        total_botoes = await botoes.count()
        logger.info(f"{total_botoes} bot√µes 'Ver an√∫ncio' encontrados na p√°gina.")

        for i in range(min(config.MAX_BOTOES_POR_PAGINA, total_botoes)):
            logger.info(f"‚û°Ô∏è  Processando an√∫ncio {i + 1}/{total_botoes}")
            botao = botoes.nth(i)

            try:
                await botao.scroll_into_view_if_needed()
                await botao.wait_for(state="attached", timeout=5000)
                await botao.hover()
                await asyncio.sleep(0.5)
                await botao.click(timeout=config.TIMEOUT_PADRAO)
                logger.info(f"‚úÖ Clicou no an√∫ncio {i + 1}")
            except Exception as e:
                logger.warning(f"‚ùå Falha ao clicar no bot√£o {i + 1}: {e}")
                continue

            await pagina.wait_for_load_state("networkidle")
            await asyncio.sleep(2.0)

            preco = await tentar_extrair_preco(pagina, config)
            logger.info(f"üí∞ Pre√ßo extra√≠do: {preco}")

            dados_tecnicos = await tentar_extrair_dados_tecnicos(pagina, config)
            logger.info(f"üìÑ Dados t√©cnicos extra√≠dos.")

            revenda = await tentar_extrair_revenda(pagina, config)
            logger.info(f"üè¢ Localiza√ß√£o da revenda: {revenda}")

            url_anuncio = pagina.url
            dados_coletados.append({
                "Pre√ßo": preco,
                **dados_tecnicos,
                "Localiza√ß√£o": revenda,
                "URL": url_anuncio
            })

            logger.info("üîô Voltando para listagem de an√∫ncios...")
            try:
                await pagina.go_back(timeout=config.TIMEOUT_PADRAO)
                await pagina.wait_for_selector('button.button', timeout=config.TIMEOUT_PADRAO)
                await asyncio.sleep(1.0)
                logger.info("‚Ü©Ô∏è Retornou com sucesso para a listagem")
            except Exception as e:
                logger.error(f"‚ùå Erro ao retornar para a listagem: {e}")
                break

    except Exception as e:
        logger.error(f"‚ùå Erro geral durante extra√ß√£o: {e}")
    return dados_coletados

async def coletar_dados_trucadao(pagina, config: Config) -> List[Dict]:
    todos_dados = []
    pagina_atual = 1

    while pagina_atual <= config.MAX_PAGINAS:
        logger.info(f"üìÑ P√°gina {pagina_atual} de {config.MAX_PAGINAS}")
        dados = await extracaoDadosTrucadao(pagina, config)
        todos_dados.extend(dados)

        try:
            proximo_botao = pagina.locator("ul.MuiPagination-ul li:has(a[aria-label='Go to next page']) a")
            await proximo_botao.wait_for(state="visible", timeout=10000)
            if await proximo_botao.is_enabled():
                await proximo_botao.click()
                await pagina.wait_for_load_state("networkidle")
                pagina_atual += 1
            else:
                logger.info("‚õî Bot√£o de pr√≥xima p√°gina indispon√≠vel.")
                break
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è Falha ao mudar de p√°gina: {e}")
            break

    return todos_dados

async def iniciar_scraping():
    async with async_playwright() as p:
        try:
            navegador = await p.chromium.launch(headless=True, slow_mo=100)
            pagina = await navegador.new_page()
            config = Config()

            logger.info(f"üåê Acessando {config.URL_BASE}")
            await pagina.goto(config.URL_BASE, timeout=60000)
            await pagina.wait_for_load_state("networkidle")

            dados = await coletar_dados_trucadao(pagina, config)

            if dados:
                df = pd.DataFrame(dados)
                df.to_excel(config.OUTPUT_FILE, index=False)
                logger.info(f"‚úÖ {len(dados)} registros salvos em {config.OUTPUT_FILE}")
            else:
                logger.warning("‚ö†Ô∏è Nenhum dado foi extra√≠do.")
        except Exception as e:
            logger.critical(f"üí• Erro cr√≠tico durante execu√ß√£o: {e}")
        finally:
            await navegador.close()

if __name__ == "__main__":
    asyncio.run(iniciar_scraping())